{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando biblotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar o conjunto de dados\n",
    "df = pd.read_csv('fer2013.csv')  # Substitua pelo caminho do seu conjunto de dados\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento do conjunto de dados\n",
    "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\n",
    "img_array = np.stack(img_array, axis=0) / 255.0\n",
    "\n",
    "labels = df.emotion.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(img_array, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(np.array(df.pixels.loc[0].split(' ')).reshape(48, 48).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4:'sadness', 5:'surprise', 6:'neutral'}\n",
    "\n",
    "fig = pyplot.figure(1, (14, 14))\n",
    "k = 0\n",
    "for label in sorted(df.emotion.unique()):\n",
    "  for j in range(3):\n",
    "    px = df[df.emotion==label].pixels.iloc[k]\n",
    "    px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n",
    "    k += 1\n",
    "    ax = pyplot.subplot(7, 7, k)\n",
    "    ax.imshow(px)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(label_to_text[label])\n",
    "    pyplot.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(img_array, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conjunto de treinamento (X_train):\")\n",
    "print(X_train)\n",
    "\n",
    "print(\"\\nConjunto de teste (X_test):\")\n",
    "print(X_test)\n",
    "\n",
    "print(\"\\nLabels do conjunto de treinamento (y_train):\")\n",
    "print(y_train)\n",
    "\n",
    "print(\"\\nLabels do conjunto de teste (y_test):\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o modelo MLP\n",
    "#Isso cria um modelo sequencial, que é uma pilha linear de camadas. Os dados fluem sequencialmente através das camadas\n",
    "model_mlp = tf.keras.models.Sequential([\n",
    "\n",
    "    #Converte os dados de entrada em um vetor 1D, nesse caso de 48x48 pixels para em escala de cinza (1)\n",
    "    tf.keras.layers.Flatten(input_shape=(48, 48, 1)),\n",
    "    #Camada densa com 128 neurônios e função de ativação relu\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    #Camada de dropout para evitar overfitting\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #Camada densa com 64 neurônios e função de ativação relu\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    #Camada de dropout para evitar overfitting\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #Camada densa com 32 neurônios e função de ativação relu\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    #Camada de dropout para evitar overfitting\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #Camada densa com 7 neurônios e função de ativação softmax, que serve para converter a saidas em probabilidades\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilando o modelo\n",
    "model_mlp.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o callback para salvar o melhor modelo durante o treinamento\n",
    "checkpoint_path = 'checkpoint/best_model_mlp.h5'\n",
    "call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                               # Parâmetro monitor: qual métrica será monitorada\n",
    "                                                monitor='val_accuracy',\n",
    "                                                # Verbose: controla a quantidade de informações impressas durante o treinamento\n",
    "                                                verbose=1,\n",
    "                                                # Salva o modelo a cada época\n",
    "                                                save_freq='epoch',\n",
    "                                                # Parâmetro save_best_only: se True, o modelo é salvo quando a métrica monitorada é maximizada\n",
    "                                                save_best_only=True,\n",
    "                                                # determina se apenas os pesos do modelo devem ser salvos ou o modelo completo, FALSE significa completo\n",
    "                                                save_weights_only=False,\n",
    "                                                # Parâmetro mode: indica se a métrica monitorada deve ser maximizada ou minimizada\n",
    "                                                mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "model_mlp.fit(X_train, y_train, epochs=20, validation_split=0.1, callbacks=[call_back])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo no conjunto de teste\n",
    "_, test_accuracy = model_mlp.evaluate(X_test, y_test)\n",
    "print(f'Acurácia no conjunto de teste: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo treinado\n",
    "final_model_mlp = load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento das labels para texto\n",
    "label_to_text = {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processamento da imagem para o modelo\n",
    "def preprocess_image(image):\n",
    "    img = Image.open(image).convert('L').resize((48, 48))\n",
    "    img_array = np.array(img).reshape(1, 48, 48, 1).astype('float32') / 255.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Função para capturar o vídeo da câmera\n",
    "def capture_video():\n",
    "    # Carregar o classificador de detecção de rosto\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)  # 0 para a câmera padrão\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Conversão para escala de cinza para detecção de rosto\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detectar rostos na imagem\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        # Iterar sobre os rostos detectados\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Recortar a região do rosto\n",
    "            face_roi = gray_frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Redimensionar a imagem facial para o tamanho esperado pelo modelo\n",
    "            resized_face = cv2.resize(face_roi, (48, 48))\n",
    "            \n",
    "            # Converter a imagem facial para escala de cinza\n",
    "            gray_face = cv2.cvtColor(resized_face, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "            # Normalizar os pixels da imagem facial\n",
    "            normalized_face = gray_face.astype('float32') / 255.0\n",
    "            \n",
    "            # Pré-processamento da imagem para o modelo de emoção\n",
    "            preprocessed_frame = normalized_face.reshape( 48, 48, 3)\n",
    "            \n",
    "            # Aplicação do modelo para obter as previsões de emoção\n",
    "            predicted_class = final_model_mlp.predict(preprocessed_frame).argmax()\n",
    "            predicted_emotion = label_to_text[predicted_class]\n",
    "            \n",
    "            # Desenhar um retângulo ao redor do rosto detectado\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            \n",
    "            # Exibir a emoção detectada sobreposta ao retângulo\n",
    "            cv2.putText(frame, predicted_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        \n",
    "        # Exibir o vídeo com as detecções de rosto e emoções\n",
    "        cv2.imshow('Emotion Recognition', frame)\n",
    "        \n",
    "        # Pressione 'q' para sair do loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Libere o objeto VideoCapture e feche a janela\n",
    "    cap.release()\n",
    "\n",
    "# Chamada da função para capturar o vídeo da câmera\n",
    "capture_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projeto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
